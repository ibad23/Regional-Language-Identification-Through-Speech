{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV for resizing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_dataset(input_folder, target_sr=16000, target_length=80000, n_mels=40, n_fft=1024, hop_length=512, top_db=20):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over each audio file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "\n",
    "        if filename.endswith((\".wav\", \".mp3\", \".webm\")):\n",
    "\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            # print(f\"Processing {filename}\")\n",
    "\n",
    "            # Step 1: Load and resample audio\n",
    "            y, sr = librosa.load(file_path, sr=target_sr)\n",
    "\n",
    "            # Step 2: Trim or pad the audio to the target length (5 seconds)\n",
    "            y = trim_or_pad_audio(y, target_length)\n",
    "\n",
    "            # Step 3: Remove silence from the audio\n",
    "            y = remove_silence(y, sr, top_db)\n",
    "\n",
    "            # Step 4: Generate a log mel-spectrogram\n",
    "            log_mel_spec = generate_mel_spectrogram(y, sr, n_mels, n_fft, hop_length)\n",
    "\n",
    "            # Step 5: Resize the spectrogram to 432x288 pixels\n",
    "            log_mel_spec_resized = resize_spectrogram(log_mel_spec)\n",
    "\n",
    "            # Store the spectrogram and corresponding label \n",
    "            spectrograms.append(log_mel_spec_resized)\n",
    "            labels.append(filename)\n",
    "            \n",
    "        # break\n",
    "    return spectrograms, labels\n",
    "\n",
    "def trim_or_pad_audio(y, target_length=80000):\n",
    "    if len(y) > target_length:\n",
    "        return y[:target_length]  # Trim to the target length\n",
    "    else:\n",
    "        return np.pad(y, (0, target_length - len(y)), 'constant')  # Pad with zeros\n",
    "\n",
    "def remove_silence(y, sr, top_db=20):\n",
    "    return librosa.effects.trim(y, top_db=top_db)[0]\n",
    "\n",
    "def generate_mel_spectrogram(y, sr, n_mels=40, n_fft=1024, hop_length=512):\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=y, \n",
    "        sr=sr, \n",
    "        n_mels=n_mels, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length,\n",
    "        fmin=20, \n",
    "        fmax=8000\n",
    "    )\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return log_mel_spec\n",
    "\n",
    "def resize_spectrogram(spec, target_size=(256, 375)): # (224, 224)\n",
    "    # Resize the spectrogram to the desired size using OpenCV\n",
    "    return cv2.resize(spec, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def display_spectrogram(spectrogram, title='Spectrogram'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    librosa.display.specshow(spectrogram, sr=16000, x_axis='time', y_axis='mel', fmin=20, fmax=8000, cmap='coolwarm')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Normalize function based on ImageNet stats (mean and std)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "def preprocess_spectrogram(spectrogram):\n",
    "    # Convert spectrogram from shape (224, 224) to (224, 224, 3)\n",
    "    \n",
    "    # First, normalize the spectrogram if necessary (e.g., if pixel range is [0, 255])\n",
    "    spectrogram = spectrogram.astype(np.float16) / 255.0\n",
    "    \n",
    "    # Replicate the spectrogram into 3 channels (to match ResNet50's expected input shape)\n",
    "    spectrogram = np.repeat(spectrogram[..., np.newaxis], 3, axis=-1) # Adds 3rd dimension  3\n",
    "   \n",
    "    spectrogram = (spectrogram - mean) / std\n",
    "    \n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del true_labels,true_labels_bin,tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset from train folder\n",
    "# train_folder = 'data/actual/train'\n",
    "# test_folder = 'data/actual/test'\n",
    "\n",
    "train_folder = 'data/first_two/train'\n",
    "test_folder = 'data/first_two/test'\n",
    "\n",
    "# train_folder = 'data/last_two/train'\n",
    "# test_folder = 'data/last_two/test'\n",
    "\n",
    "\n",
    "# Example for one language folder, say 'english' in the train folder\n",
    "language_1 = 'punjabi' \n",
    "train_path_1 = os.path.join(train_folder, language_1)\n",
    "#test_path_1 = os.path.join(test_folder, language_1)\n",
    "\n",
    "language_2 = 'pushto' \n",
    "train_path_2 = os.path.join(train_folder, language_2)\n",
    "#test_path_2 = os.path.join(test_folder, language_2)\n",
    "\n",
    "language_3 = 'saraiki' \n",
    "train_path_3 = os.path.join(train_folder, language_3)\n",
    "#test_path_3 = os.path.join(test_folder, language_3)\n",
    "\n",
    "language_4 = 'sindhi' \n",
    "train_path_4 = os.path.join(train_folder, language_4)\n",
    "#test_path_4 = os.path.join(test_folder, language_4)\n",
    "\n",
    "language_5 = 'urdu'  \n",
    "train_path_5 = os.path.join(train_folder, language_5)\n",
    "#test_path_5 = os.path.join(test_folder, language_5)\n",
    "\n",
    "\n",
    "# Preprocess the train dataset for the language\n",
    "punjabi_spectrograms, punjabi_labels = preprocess_audio_dataset(train_path_1)\n",
    "pushto_spectrograms, pushto_labels = preprocess_audio_dataset(train_path_2)\n",
    "saraiki_spectrograms, saraiki_labels = preprocess_audio_dataset(train_path_3)\n",
    "sindhi_spectrograms, sindhi_labels = preprocess_audio_dataset(train_path_4)\n",
    "urdu_spectrograms, urdu_labels = preprocess_audio_dataset(train_path_5)\n",
    "\n",
    "train_spectrograms = punjabi_spectrograms + sindhi_spectrograms + pushto_spectrograms + saraiki_spectrograms + urdu_spectrograms\n",
    "\n",
    "train_labels = ['punjabi'] * len(punjabi_spectrograms) + \\\n",
    "         ['sindhi'] * len(sindhi_spectrograms) + \\\n",
    "         ['pushto'] * len(pushto_spectrograms) + \\\n",
    "         ['saraiki'] * len(saraiki_spectrograms) + \\\n",
    "         ['urdu'] * len(urdu_spectrograms)\n",
    "\n",
    "assert len(train_spectrograms) == len(train_labels), \"Mismatch between spectrograms and labels count.\"\n",
    "\n",
    "print(\"Total Spectrograms: \", len(train_spectrograms))\n",
    "\n",
    "# Display one spectrogram as a sample\n",
    "# display_spectrogram(train_spectrograms[0], title=\"train Spectrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del punjabi_spectrograms,sindhi_spectrograms,pushto_spectrograms,saraiki_spectrograms,urdu_spectrograms\n",
    "del saraiki_labels,urdu_labels,punjabi_labels,sindhi_labels,pushto_labels\n",
    "del sindhi_spectrograms,punjabi_spectrograms,pushto_spectrograms,urdu_spectrograms,saraiki_spectrograms\n",
    "del train_folder,test_folder \n",
    "del train_path_1,train_path_2,train_path_3,train_path_4,train_path_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Train Spectrograms Shape: {train_spectrograms.shape}\")\n",
    "# Preprocess training and testing spectrograms\n",
    "train_spectrograms = np.array([preprocess_spectrogram(s) for s in train_spectrograms])\n",
    "#test_spectrograms = np.array([preprocess_spectrogram(s) for s in test_spectrograms])\n",
    "\n",
    "# Ensure spectrograms are the correct shape\n",
    "print(f\"Train Spectrograms Shape: {train_spectrograms.shape}\")\n",
    "#print(f\"Test Spectrograms Shape: {test_spectrograms.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure spectrograms are the correct shape\n",
    "print(f\"Train Spectrograms Shape: {train_spectrograms.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Encode labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)  # Fit and transform the training labels\n",
    "\n",
    "# Print label mapping for reference\n",
    "print(\"Label Mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "train_spectrograms, val_spectrograms, train_labels_encoded, val_labels_encoded = train_test_split(\n",
    "    train_spectrograms, train_labels_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Create TensorFlow Dataset objects for train and test datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_spectrograms, train_labels_encoded))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_spectrograms, val_labels_encoded))\n",
    "\n",
    "\n",
    "# Shuffle and batch the train dataset\n",
    "train_dataset = train_dataset.shuffle(len(train_spectrograms)).batch(16)\n",
    "\n",
    "# Batch the validation and test datasets (no shuffle needed)\n",
    "val_dataset = val_dataset.batch(16)\n",
    "\n",
    "# 4. Inspect the dataset (first batch example)\n",
    "for images, labels in train_dataset.take(1):  # Just take 1 batch to inspect\n",
    "    print(\"Batch image shape:\", images.shape)\n",
    "    print(\"Batch labels shape:\", labels.shape)\n",
    "\n",
    "# Now you can feed these datasets to your model during training\n",
    "# Example:\n",
    "# model.fit(train_dataset, validation_data=test_dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(375, 256, 3)),\n",
    "        layers.Normalization(axis=-1),\n",
    "        layers.Conv2D(16, (7, 5), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((3, 2)),\n",
    "        layers.Conv2D(32, (5, 5), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(5, activation='softmax')  # Output layer for classification\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Input shape of spectrograms (224, 224, 3)\n",
    "input_shape = (375, 256, 3)\n",
    "num_classes = 5  # 5 languages\n",
    "\n",
    "# Create the CNN model\n",
    "model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,  # Stop after 2 epochs without improvement\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "class BatchLossLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batch_losses = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_losses.append(logs['loss'])\n",
    "\n",
    "batch_logger1 = BatchLossLogger()\n",
    "checkpoint = ModelCheckpoint(\n",
    "filepath='best_model_pt1.keras',  # Save file path\n",
    "monitor='val_loss',       # Metric to monitor (e.g., 'val_loss' or 'val_accuracy')\n",
    "save_best_only=True,      # Save only the model with the best performance\n",
    "mode='min',               # Mode for monitoring ('min' for loss, 'max' for accuracy)\n",
    "verbose=1                 # Verbosity level (1 to print saving logs)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history1=model.fit(\n",
    "train_dataset,  # Replace with the 4th folder's training dataset\n",
    "validation_data=val_dataset,  # Replace with the 4th folder's validation dataset\n",
    "epochs=20,  # Reduced maximum number of epochs\n",
    "callbacks=[lr_schedule,early_stopping,checkpoint,batch_logger1\n",
    "],\n",
    "verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_labels,train_labels_encoded,train_spectrograms\n",
    "# del val_labels_encoded,val_spectrograms\n",
    "# del early_stopping,checkpoint,images,input_shape,labels,num_classes\n",
    "import gc\n",
    "gc.collect()\n",
    "#use traindataset and val dataset to train next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset from train folder\n",
    "# train_folder = 'data/actual/train'\n",
    "# test_folder = 'data/actual/test'\n",
    "\n",
    "train_folder_2 = 'data/last_two/train'\n",
    "test_folder_2 = 'data/last_two/test'\n",
    "\n",
    "# Example for one language folder, say 'english' in the train folder\n",
    "#language_1_2 = 'punjabi' \n",
    "train_path_12 = os.path.join(train_folder_2, language_1)\n",
    "#test_path_1 = os.path.join(test_folder, language_1)\n",
    "\n",
    "#language_2 = 'pushto' \n",
    "train_path_22 = os.path.join(train_folder_2, language_2)\n",
    "#test_path_2 = os.path.join(test_folder, language_2)\n",
    "\n",
    "#language_3 = 'saraiki' \n",
    "train_path_32 = os.path.join(train_folder_2, language_3)\n",
    "#test_path_3 = os.path.join(test_folder, language_3)\n",
    "\n",
    "#language_4 = 'sindhi' \n",
    "train_path_42 = os.path.join(train_folder_2, language_4)\n",
    "#test_path_4 = os.path.join(test_folder, language_4)\n",
    "\n",
    "#language_5 = 'urdu'  \n",
    "train_path_52 = os.path.join(train_folder_2, language_5)\n",
    "#test_path_5 = os.path.join(test_folder, language_5)\n",
    "\n",
    "\n",
    "# Preprocess the train dataset for the language\n",
    "punjabi_spectrograms_2, punjabi_labels = preprocess_audio_dataset(train_path_12)\n",
    "pushto_spectrograms_2, pushto_labels = preprocess_audio_dataset(train_path_22)\n",
    "saraiki_spectrograms_2, saraiki_labels = preprocess_audio_dataset(train_path_32)\n",
    "sindhi_spectrograms_2, sindhi_labels = preprocess_audio_dataset(train_path_42)\n",
    "urdu_spectrograms_2, urdu_labels = preprocess_audio_dataset(train_path_52)\n",
    "\n",
    "train_spectrograms_2 = punjabi_spectrograms_2 + sindhi_spectrograms_2 + pushto_spectrograms_2 + saraiki_spectrograms_2 + urdu_spectrograms_2\n",
    "\n",
    "train_labels_2 = ['punjabi'] * len(punjabi_spectrograms_2) + \\\n",
    "         ['sindhi'] * len(sindhi_spectrograms_2) + \\\n",
    "         ['pushto'] * len(pushto_spectrograms_2) + \\\n",
    "         ['saraiki'] * len(saraiki_spectrograms_2) + \\\n",
    "         ['urdu'] * len(urdu_spectrograms_2)\n",
    "\n",
    "assert len(train_spectrograms_2) == len(train_labels_2), \"Mismatch between spectrograms and labels count.\"\n",
    "\n",
    "print(\"Total Spectrograms: \", len(train_spectrograms_2))\n",
    "train_spectrograms_2 = np.array([preprocess_spectrogram(s) for s in train_spectrograms_2])\n",
    "#test_spectrograms = np.array([preprocess_spectrogram(s) for s in test_spectrograms])\n",
    "\n",
    "# Ensure spectrograms are the correct shape\n",
    "print(f\"Train Spectrograms Shape: {train_spectrograms_2.shape}\")\n",
    "# Display one spectrogram as a sample\n",
    "# display_spectrogram(train_spectrograms[0], title=\"train Spectrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del punjabi_spectrograms,sindhi_spectrograms,pushto_spectrograms,saraiki_spectrograms,urdu_spectrograms\n",
    "import gc\n",
    "del saraiki_labels,urdu_labels,punjabi_labels,sindhi_labels,pushto_labels\n",
    "del sindhi_spectrograms_2,punjabi_spectrograms_2,pushto_spectrograms_2,urdu_spectrograms_2,saraiki_spectrograms_2\n",
    "del train_folder_2,test_folder_2 \n",
    "del train_path_12,train_path_22,train_path_32,train_path_42,train_path_52\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Encode labels using LabelEncoder\n",
    "#label_encoder = LabelEncoder()\n",
    "train_labels_encoded_2 = label_encoder.fit_transform(train_labels_2)  # Fit and transform the training labels\n",
    "\n",
    "# Print label mapping for reference\n",
    "print(\"Label Mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "train_spectrograms_2, val_spectrograms_2, train_labels_encoded_2, val_labels_encoded_2 = train_test_split(\n",
    "    train_spectrograms_2, train_labels_encoded_2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Create TensorFlow Dataset objects for train and test datasets\n",
    "train_dataset_2 = tf.data.Dataset.from_tensor_slices((train_spectrograms_2, train_labels_encoded_2))\n",
    "val_dataset_2 = tf.data.Dataset.from_tensor_slices((val_spectrograms_2, val_labels_encoded_2))\n",
    "\n",
    "\n",
    "# Shuffle and batch the train dataset\n",
    "train_dataset_2 = train_dataset_2.shuffle(len(train_spectrograms_2)).batch(16)\n",
    "\n",
    "# Batch the validation and test datasets (no shuffle needed)\n",
    "val_dataset_2 = val_dataset_2.batch(16)\n",
    "\n",
    "# 4. Inspect the dataset (first batch example)\n",
    "for images, labels in train_dataset_2.take(1):  # Just take 1 batch to inspect\n",
    "    print(\"Batch image shape:\", images.shape)\n",
    "    print(\"Batch labels shape:\", labels.shape)\n",
    "\n",
    "# Now you can feed these datasets to your model during training\n",
    "# Example:\n",
    "# model.fit(train_dataset, validation_data=test_dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_labels_2,train_labels_encoded_2,train_spectrograms_2\n",
    "del val_labels_encoded_2,val_spectrograms_2\n",
    "del images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "#model = load_model('cnn_pt1.keras')\n",
    "model = load_model('best_model_pt1.keras')\n",
    "\n",
    "# Load the model trained on the 3rd folder\n",
    "\n",
    "early_stopping_2 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,  # Stop after 2 epochs without improvement\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "lr_schedule_2 = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "batch_logger_2 = BatchLossLogger()\n",
    "checkpoint_2 = ModelCheckpoint(\n",
    "filepath='best_model_pt2.keras',  # Save file path\n",
    "monitor='val_loss',       # Metric to monitor (e.g., 'val_loss' or 'val_accuracy')\n",
    "save_best_only=True,      # Save only the model with the best performance\n",
    "mode='min',               # Mode for monitoring ('min' for loss, 'max' for accuracy)\n",
    "verbose=1                 # Verbosity level (1 to print saving logs)\n",
    ")\n",
    "# Train the model\n",
    "# Train the model\n",
    "model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "history2=model.fit(\n",
    "train_dataset_2,  # Replace with the 4th folder's training dataset\n",
    "validation_data=val_dataset_2,  # Replace with the 4th folder's validation dataset\n",
    "epochs=20,  # Reduced maximum number of epochs\n",
    "callbacks=[lr_schedule_2,early_stopping_2,checkpoint_2,batch_logger_2\n",
    "],\n",
    "verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loss = history1.history['loss'] + history2.history['loss']\n",
    "combined_val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
    "#96 train acc 80 valid acc for best model pt1 : its bestmodelpt2\n",
    "#97 train acc 71 valid acc cnnpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, len(combined_loss) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, combined_loss, label='Training Loss')\n",
    "plt.plot(epochs, combined_val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine batch losses\n",
    "combined_batch_losses = batch_logger1.batch_losses + batch_logger_2.batch_losses\n",
    "steps = list(range(1, len(combined_batch_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, combined_batch_losses, label='Loss Per Step')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_dataset,val_dataset_2,train_dataset,train_dataset_2,train_labels,train_labels_encoded,train_spectrograms,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset from train folder\n",
    "\n",
    "test_folder = 'data/first_two/test'\n",
    "test_folder_2 = 'data/last_two/test'\n",
    "\n",
    "# Example for one language folder, say 'english' in the train folder\n",
    "language_1 = 'punjabi' \n",
    "\n",
    "test_path_1 = os.path.join(test_folder, language_1)\n",
    "test_path_12 = os.path.join(test_folder_2, language_1)\n",
    "\n",
    "\n",
    "language_2 = 'pushto' \n",
    "\n",
    "test_path_2 = os.path.join(test_folder, language_2)\n",
    "test_path_22 = os.path.join(test_folder_2, language_2)\n",
    "\n",
    "language_3 = 'saraiki' \n",
    "\n",
    "test_path_3 = os.path.join(test_folder, language_3)\n",
    "test_path_32 = os.path.join(test_folder_2, language_3)\n",
    "\n",
    "language_4 = 'sindhi' \n",
    "\n",
    "test_path_4 = os.path.join(test_folder, language_4)\n",
    "test_path_42 = os.path.join(test_folder_2, language_4)\n",
    "\n",
    "language_5 = 'urdu'  \n",
    "\n",
    "test_path_5 = os.path.join(test_folder, language_5)\n",
    "test_path_52 = os.path.join(test_folder_2, language_5)\n",
    "\n",
    "# Preprocess the test dataset for the language\n",
    "test_punjabi_spectrograms, test_punjabi_labels = preprocess_audio_dataset(test_path_1)\n",
    "test_pushto_spectrograms, test_pushto_labels = preprocess_audio_dataset(test_path_2)\n",
    "test_saraiki_spectrograms, test_saraiki_labels = preprocess_audio_dataset(test_path_3)\n",
    "test_sindhi_spectrograms, test_sindhi_labels = preprocess_audio_dataset(test_path_4)\n",
    "test_urdu_spectrograms, test_urdu_labels = preprocess_audio_dataset(test_path_5)\n",
    "\n",
    "test2_punjabi_spectrograms, test2_punjabi_labels = preprocess_audio_dataset(test_path_12)\n",
    "test2_pushto_spectrograms, test2_pushto_labels = preprocess_audio_dataset(test_path_22)\n",
    "test2_saraiki_spectrograms, test2_saraiki_labels = preprocess_audio_dataset(test_path_32)\n",
    "test2_sindhi_spectrograms, test2_sindhi_labels = preprocess_audio_dataset(test_path_42)\n",
    "test2_urdu_spectrograms, test2_urdu_labels = preprocess_audio_dataset(test_path_52)\n",
    "\n",
    "\n",
    "test_spectrograms = test_punjabi_spectrograms+ test2_punjabi_spectrograms+ test_sindhi_spectrograms+test2_sindhi_spectrograms + test_pushto_spectrograms +test2_pushto_spectrograms+ test_saraiki_spectrograms+test2_saraiki_spectrograms  + test_urdu_spectrograms+test2_urdu_spectrograms \n",
    "\n",
    "test_labels = [0] * len(test_punjabi_spectrograms+test2_punjabi_spectrograms) + \\\n",
    "         [3] * len(test_sindhi_spectrograms+test2_sindhi_spectrograms) + \\\n",
    "         [1] * len(test_pushto_spectrograms+test2_pushto_spectrograms) + \\\n",
    "         [2] * len(test_saraiki_spectrograms+test2_saraiki_spectrograms ) + \\\n",
    "         [4] * len(test_urdu_spectrograms+test2_urdu_spectrograms )\n",
    "assert len(test_spectrograms) == len(test_labels), \"Mismatch between spectrograms and labels count.\"\n",
    "\n",
    "print(\"Total Spectrograms: \", len(test_spectrograms))\n",
    "print(\"Total Labels: \", len(test_labels))\n",
    "test_spectrograms = np.array([preprocess_spectrogram(s) for s in test_spectrograms])\n",
    "\n",
    "# Ensure spectrograms are the correct shape\n",
    "\n",
    "print(f\"Test Spectrograms Shape: {test_spectrograms.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del language_1,language_2,language_3,language_4,language_5\n",
    "# del test_folder,test_folder_2\n",
    "# del test_path_1,test_path_12,test2_urdu_spectrograms,test2_punjabi_labels,test2_pushto_labels\n",
    "# del test_path_2,test_path_22,test_path_3,test_path_32,test_path_4,test_path_42,test_path_5,test_path_52\n",
    "# del test_punjabi_labels,test_punjabi_spectrograms,test_pushto_labels,test_pushto_spectrograms,test_saraiki_labels,test_saraiki_spectrograms\n",
    "# del test_sindhi_labels,test_sindhi_spectrograms,test_urdu_labels,test_urdu_spectrograms\n",
    "# del test2_punjabi_spectrograms,test2_pushto_spectrograms,test2_saraiki_labels,test2_saraiki_spectrograms,test2_sindhi_labels\n",
    "# del test2_sindhi_spectrograms,test2_urdu_labels\n",
    "# del batch_logger1,batch_logger_2    \n",
    "# del combined_batch_losses,combined_loss,combined_val_loss\n",
    "# del epochs,history1,history2,language_1\n",
    "del language_2,language_3,language_4,language_5,steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_entire_testset(model, test_spectrograms, test_labels, labels_class):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the entire test set and computes the accuracy, confusion matrix,\n",
    "    and a classification report.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to evaluate.\n",
    "        test_spectrograms: Array of test spectrograms.\n",
    "        test_labels: Array of corresponding test labels.\n",
    "        labels_class: List of all possible class labels (e.g., [0, 1, 2, 3, 4]).\n",
    "\n",
    "    Returns:\n",
    "        overall_accuracy: The accuracy of the model on the test set.\n",
    "        cm: The confusion matrix for the test set.\n",
    "    \"\"\"\n",
    "    # Convert test data into a TensorFlow Dataset\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_spectrograms, test_labels))\n",
    "    test_dataset = test_dataset.batch(32)  # Batch size\n",
    "\n",
    "    # Predictions and true labels\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for images, labels in test_dataset:\n",
    "        # Predict the labels\n",
    "        preds = model.predict(images)\n",
    "        predicted_labels = np.argmax(preds, axis=1)  # Get the predicted class\n",
    "        predictions.extend(predicted_labels)\n",
    "        true_labels.extend(labels.numpy())  # Convert true labels to numpy array\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    overall_accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=labels_class)\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels_class, yticklabels=labels_class)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix for Entire Test Set')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the classification report\n",
    "    report = classification_report(true_labels, predictions, target_names=[f'Class {label}' for label in labels_class])\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return overall_accuracy, cm\n",
    "\n",
    "\n",
    "# Example usage\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('best_model_pt2.keras')  # Load your model\n",
    "\n",
    "# Assuming `test_spectrograms` and `test_labels` are already defined\n",
    "labels_class = [0, 1, 2, 3, 4]  # All possible classes\n",
    "overall_accuracy, cm = evaluate_entire_testset(model, test_spectrograms, test_labels, labels_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming test_dataset is your tf.data.Dataset, and the model is already trained\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_spectrograms, test_labels))\n",
    "test_dataset = test_dataset.batch(16)  \n",
    "# Initialize lists to store the true labels and predicted probabilities\n",
    "true_labels = []\n",
    "pred_probs = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for spectrograms, labels in test_dataset:\n",
    "    # Predict probabilities for the batch\n",
    "    batch_pred_probs = model.predict(spectrograms)\n",
    "    \n",
    "    # Store the true labels and predicted probabilities\n",
    "    true_labels.append(labels.numpy())  # Convert Tensor to numpy array\n",
    "    pred_probs.append(batch_pred_probs)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "pred_probs = np.concatenate(pred_probs, axis=0)\n",
    "\n",
    "# Binarize the true labels for ROC\n",
    "true_labels_bin = label_binarize(true_labels, classes=[0, 1, 2, 3, 4])\n",
    "n_classes = true_labels_bin.shape[1]\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr, tpr, _ = roc_curve(true_labels_bin[:, i], pred_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "# Diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Test Dataset')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del images,input_shape,label_encoder,labels,labels_class,model,n_classes,num_classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
